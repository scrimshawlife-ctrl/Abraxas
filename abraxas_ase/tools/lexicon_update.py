from __future__ import annotations

import argparse
import datetime as dt
import hashlib
import json
import re
from pathlib import Path
from typing import Dict, Iterable, List, Sequence, Tuple

from abraxas_ase import __version__

ASCII_FOLD_RE = re.compile(r"[^\x00-\x7F]+")
TOKEN_STRIP_RE = re.compile(r"^[^a-zA-Z]+|[^a-zA-Z]+$")


def _ascii_fold(s: str) -> str:
    # Deterministic simple ASCII fold: drop non-ascii.
    # (If you prefer NFKD fold, keep it consistent across environments; this is stable.)
    return ASCII_FOLD_RE.sub("", s)


def _normalize_token(s: str) -> str:
    s = s.strip()
    s = _ascii_fold(s)
    s = s.lower()
    s = TOKEN_STRIP_RE.sub("", s)
    s = " ".join(s.split())
    return s


def _is_valid_token(s: str, min_len: int) -> bool:
    if len(s) < min_len:
        return False
    if not s.isalpha():
        return False
    return True


def _sha256_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()


def _stable_json(obj) -> str:
    return json.dumps(obj, sort_keys=True, separators=(",", ":"), ensure_ascii=True)


def _read_tokens_file(path: Path, min_len: int) -> List[str]:
    toks: List[str] = []
    raw = path.read_text(encoding="utf-8").splitlines()
    for line in raw:
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        t = _normalize_token(line)
        if _is_valid_token(t, min_len=min_len):
            toks.append(t)
    # deterministic: sort & dedupe
    return sorted(set(toks))


def _discover_sources(in_dir: Path) -> List[Path]:
    # deterministic ordering
    return sorted([p for p in in_dir.glob("*.txt") if p.is_file()], key=lambda p: p.name)


def _partition_sources(paths: List[Path]) -> Tuple[List[Path], List[Path]]:
    # stopwords: filenames containing 'stop'
    stop = [p for p in paths if "stop" in p.name.lower()]
    subs = [p for p in paths if p not in stop]
    return stop, subs


def _emit_generated_py(out_pkg_dir: Path, stopwords: List[str], subwords: List[str], manifest_hash: str) -> str:
    # Use tuples for stable representation, then frozenset(tuple) deterministically.
    # Keep file content byte-stable.
    lines: List[str] = []
    lines.append("# AUTO-GENERATED BY abraxas_ase.tools.lexicon_update")
    lines.append(f"# manifest_sha256: {manifest_hash}")
    lines.append("# DO NOT EDIT BY HAND.\n")
    lines.append("STOPWORDS_TUPLE = (")
    for w in stopwords:
        lines.append(f"    {w!r},")
    lines.append(")\n")
    lines.append("SUBWORDS_TUPLE = (")
    for w in subwords:
        lines.append(f"    {w!r},")
    lines.append(")\n")
    lines.append("STOPWORDS = frozenset(STOPWORDS_TUPLE)")
    lines.append("SUBWORDS = frozenset(SUBWORDS_TUPLE)\n")
    content = "\n".join(lines) + "\n"
    return content


def _write_if_changed(path: Path, content: str) -> bool:
    if path.exists():
        existing = path.read_text(encoding="utf-8")
        if existing == content:
            return False
    path.write_text(content, encoding="utf-8", newline="\n")
    return True


def _build_manifest(
    in_dir: Path,
    stop_sources: List[Path],
    sub_sources: List[Path],
    per_source_tokens: Dict[str, List[str]],
    stopwords: List[str],
    subwords: List[str],
    now_utc_iso: str,
) -> Dict:
    per_file_hash = {}
    per_file_count = {}
    for p in (stop_sources + sub_sources):
        b = p.read_bytes()
        per_file_hash[p.name] = _sha256_bytes(b)
        per_file_count[p.name] = len(per_source_tokens[p.name])

    merged_stop_hash = _sha256_bytes(("\n".join(stopwords) + "\n").encode("utf-8"))
    merged_sub_hash = _sha256_bytes(("\n".join(subwords) + "\n").encode("utf-8"))

    return {
        "tool": "abraxas-ase.lexicon_update",
        "tool_version": __version__,
        "generated_at_utc": now_utc_iso,
        "inputs_dir": str(in_dir.as_posix()),
        "files": {
            "sha256": per_file_hash,
            "counts": per_file_count,
        },
        "merged": {
            "stopwords_count": len(stopwords),
            "subwords_count": len(subwords),
            "stopwords_sha256": merged_stop_hash,
            "subwords_sha256": merged_sub_hash,
        },
    }


def _diff_summary(old: List[str], new: List[str]) -> Dict[str, List[str]]:
    os = set(old)
    ns = set(new)
    added = sorted(ns - os)
    removed = sorted(os - ns)
    return {"added": added, "removed": removed}


def main() -> None:
    ap = argparse.ArgumentParser(prog="python -m abraxas_ase.tools.lexicon_update")
    ap.add_argument("--in", dest="in_dir", required=True, help="Input directory with *.txt sources")
    ap.add_argument("--out", dest="out_dir", required=True, help="Output package dir (e.g. abraxas_ase)")
    ap.add_argument("--min-len", dest="min_len", type=int, default=3)
    ap.add_argument("--check", dest="check", action="store_true", help="Do not write; fail if generated differs")
    ap.add_argument("--report", dest="report", action="store_true", help="Print deterministic diff summary")
    args = ap.parse_args()

    in_dir = Path(args.in_dir)
    out_dir = Path(args.out_dir)

    if not in_dir.exists() or not in_dir.is_dir():
        raise SystemExit(f"--in must be an existing directory: {in_dir}")

    sources = _discover_sources(in_dir)
    if not sources:
        raise SystemExit(f"No *.txt sources found in {in_dir}")

    stop_src, sub_src = _partition_sources(sources)

    per_source_tokens: Dict[str, List[str]] = {}
    stopwords_all: List[str] = []
    subwords_all: List[str] = []

    for p in stop_src:
        toks = _read_tokens_file(p, min_len=args.min_len)
        per_source_tokens[p.name] = toks
        stopwords_all.extend(toks)

    for p in sub_src:
        toks = _read_tokens_file(p, min_len=args.min_len)
        per_source_tokens[p.name] = toks
        subwords_all.extend(toks)

    # Merge deterministically
    stopwords = sorted(set(stopwords_all))
    subwords = sorted(set(subwords_all))

    now = dt.datetime.now(dt.timezone.utc).replace(microsecond=0).isoformat()

    manifest_obj = _build_manifest(
        in_dir=in_dir,
        stop_sources=stop_src,
        sub_sources=sub_src,
        per_source_tokens=per_source_tokens,
        stopwords=stopwords,
        subwords=subwords,
        now_utc_iso=now,
    )
    manifest_json = _stable_json(manifest_obj) + "\n"
    manifest_hash = _sha256_bytes(manifest_json.encode("utf-8"))

    gen_py = _emit_generated_py(out_dir, stopwords, subwords, manifest_hash)

    gen_path = out_dir / "lexicon_generated.py"
    man_path = out_dir / "lexicon_manifest.json"

    if args.report and gen_path.exists() and man_path.exists():
        # Deterministic report by comparing merged lists
        old_gen = (gen_path.read_text(encoding="utf-8").splitlines())
        # Extract old tuples cheaply (robustness: keep it simple & deterministic)
        # If parsing fails, still proceed with regeneration.
        def _extract_tuple(lines: List[str], name: str) -> List[str]:
            start = None
            end = None
            for i, ln in enumerate(lines):
                if ln.strip() == f"{name}_TUPLE = (":
                    start = i + 1
                    continue
                if start is not None and ln.strip() == ")":
                    end = i
                    break
            if start is None or end is None:
                return []
            vals = []
            for ln in lines[start:end]:
                s = ln.strip().rstrip(",")
                if s.startswith("'") or s.startswith('"'):
                    vals.append(eval(s))  # deterministic, controlled file
            return vals

        old_stop = _extract_tuple(old_gen, "STOPWORDS")
        old_sub = _extract_tuple(old_gen, "SUBWORDS")
        rep = {
            "stopwords": _diff_summary(old_stop, stopwords),
            "subwords": _diff_summary(old_sub, subwords),
            "counts": {"stopwords": len(stopwords), "subwords": len(subwords)},
            "manifest_sha256": manifest_hash,
        }
        print(_stable_json(rep))

    if args.check:
        # Compute whether writing would change files
        would_change = False
        if not gen_path.exists() or gen_path.read_text(encoding="utf-8") != gen_py:
            would_change = True
        if not man_path.exists() or man_path.read_text(encoding="utf-8") != manifest_json:
            would_change = True
        if would_change:
            raise SystemExit("Lexicon generated artifacts are stale. Run without --check to regenerate.")
        return

    wrote_gen = _write_if_changed(gen_path, gen_py)
    wrote_man = _write_if_changed(man_path, manifest_json)

    # Deterministic summary line
    changed = []
    if wrote_gen:
        changed.append("lexicon_generated.py")
    if wrote_man:
        changed.append("lexicon_manifest.json")
    print(_stable_json({
        "status": "ok",
        "changed": changed,
        "manifest_sha256": manifest_hash,
        "stopwords": len(stopwords),
        "subwords": len(subwords),
    }))


if __name__ == "__main__":
    main()
