<div align="center">

# ğŸœ Abraxas

### Deterministic Symbolic Intelligence & Linguistic Weather System

*Provenance-embedded compression detection, memetic drift analysis, and self-healing infrastructure for edge deployment*

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Node.js 18+](https://img.shields.io/badge/node-18+-green.svg)](https://nodejs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.6-blue)](https://www.typescriptlang.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

[Quick Start](#-quick-start) â€¢ [Architecture](#-architecture) â€¢ [Features](#-features) â€¢ [Documentation](#-documentation) â€¢ [Project Status](#-project-status)

</div>

---

## ğŸ¯ What is Abraxas?

**Abraxas** is a production-grade symbolic intelligence system that detects linguistic compression patterns, tracks memetic drift, and operates as an always-on edge appliance with self-healing capabilities.

Think of it as a **weather system for language** â€” detecting when symbols compress ("eggcorns" like "apex twin" â†’ "aphex twin"), tracking affective drift, and generating deterministic provenance for every linguistic event.

### Core Capabilities

- **ğŸ”¬ Symbolic Compression Detection (SCO/ECO)** â€” Detect and quantify when opaque symbols are replaced with semantically transparent substitutes
- **ğŸŒ¦ï¸ Weather Engine** â€” Transform linguistic events into memetic weather patterns and drift signals
- **ğŸ“Š Scenario Envelope Runner (SER)** â€” Deterministic forecasting driven by simulation priors; generates cascade sheets and contamination advisories without requiring full simulation
- **ğŸ¤– Always-On Daemon** â€” Continuous data ingestion via Decodo API with chat-like interaction interface
- **ğŸ›¡ï¸ Self-Healing Infrastructure** â€” Drift detection, watchdog monitoring, and atomic updates with rollback
- **âš¡ Orin-Ready Edge Deployment** â€” Optimized for NVIDIA Jetson Orin with systemd integration
- **ğŸ”’ Provenance-First Design** â€” Every event includes SHA-256 hash for reproducibility and auditability

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# System requirements
- Python 3.11+
- Node.js 18+
- (Optional) NVIDIA Jetson Orin for edge deployment
```

### Installation

```bash
# Clone repository
git clone https://github.com/scrimshawlife-ctrl/Abraxas.git
cd Abraxas

# Install Python dependencies
pip install -e .

# Install Node.js dependencies
npm install

# Run system diagnostic
abx doctor
```

### Run Your First Analysis

```bash
# Analyze text for symbolic compression
python -m abraxas.cli.sco_run \
  --records tests/records.json \
  --lexicon tests/lexicon.json \
  --out events.jsonl \
  --domain music

# Start the always-on daemon
abx ui

# Start continuous ingestion (requires Decodo credentials)
abx ingest
```

### Development Server

```bash
# Start TypeScript development server
npm run dev

# Run tests
npm test
pytest tests/
```

---

## ğŸ—ï¸ Architecture

Abraxas operates as a **multi-layer stack** combining Python linguistic analysis with TypeScript orchestration:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ABRAXAS ECOSYSTEM                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  TypeScript/Express Layer (Node.js)                      â”‚  â”‚
â”‚  â”‚  â€¢ API Routes & Express Server                           â”‚  â”‚
â”‚  â”‚  â€¢ Weather Engine Integration                            â”‚  â”‚
â”‚  â”‚  â€¢ Chat UI & Admin Handshake                             â”‚  â”‚
â”‚  â”‚  â€¢ Task Registry & ERS Scheduling                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                   â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Python SCO/ECO Core                                     â”‚  â”‚
â”‚  â”‚  â€¢ Symbolic Compression Operator                         â”‚  â”‚
â”‚  â”‚  â€¢ Phonetic & Semantic Analysis                          â”‚  â”‚
â”‚  â”‚  â€¢ Transparency Index (STI) Calculation                  â”‚  â”‚
â”‚  â”‚  â€¢ Replacement Direction Vector (RDV)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Orin Boot Spine (Edge Infrastructure)                   â”‚  â”‚
â”‚  â”‚  â€¢ Drift Detection & Health Monitoring                   â”‚  â”‚
â”‚  â”‚  â€¢ Overlay Lifecycle Management                          â”‚  â”‚
â”‚  â”‚  â€¢ Atomic Updates with Rollback                          â”‚  â”‚
â”‚  â”‚  â€¢ Systemd Integration                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Data Layer                                              â”‚  â”‚
â”‚  â”‚  â€¢ Decodo Web Scraping API Integration                  â”‚  â”‚
â”‚  â”‚  â€¢ SQLite Storage with Provenance                        â”‚  â”‚
â”‚  â”‚  â€¢ JSONL Event Persistence                               â”‚  â”‚
â”‚  â”‚  â€¢ AAlmanac Ledger                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **SCO/ECO Stack** | Python | Linguistic compression detection & analysis |
| **Weather Engine** | TypeScript | Memetic drift pattern generation |
| **Orin Spine** | Python | Edge deployment infrastructure |
| **Chat UI** | Express/React | Admin interface & module discovery |
| **Ingestion Engine** | Python | Continuous data acquisition via Decodo |
| **Self-Healing Layer** | Python/Systemd | Watchdog, drift detection, atomic updates |

---

## âš¡ Features

### Symbolic Compression Detection

Detect when language users compress symbols while preserving intent:

- **ECO_T1 (Eggcorn)** â€” High phonetic similarity (â‰¥0.85) + semantic transparency delta (â‰¥0.18)
- **SCO_T2 (General Compression)** â€” Moderate thresholds with provenance tracking
- **Metrics** â€” STI, CP, IPS, SLC, RDV with deterministic scoring

Example detected event:
```json
{
  "tier": "ECO_T1",
  "original_token": "aphex twin",
  "replacement_token": "apex twin",
  "compression_pressure": 1.42,
  "symbolic_transparency_index": 0.45,
  "rdv": {
    "humor": 0.0,
    "intimacy": 0.6,
    "irony": 0.0
  }
}
```

### Weather Engine Integration

Transform compression events into memetic weather patterns:

- **Symbolic Drift** â€” Intensity of symbol replacement
- **Transparency Flux** â€” Rate of semantic clarification/obscuration
- **RDV Tracking** â€” Humor, aggression, authority, intimacy, nihilism, irony
- **Compression Stability** â€” Eggcorn formation rate

### Lexicon Engine v1

Domain-scoped, versioned token-weight mapping with deterministic compression:

```python
from abraxas.lexicon import LexiconEngine, LexiconPack, LexiconEntry, InMemoryLexiconRegistry

# Create a lexicon pack
pack = LexiconPack(
    domain="slang",
    version="1.0.0",
    entries=(
        LexiconEntry("cap", 0.9, {"tag": "negation"}),
        LexiconEntry("no_cap", 1.1, {"tag": "assertion"}),
    ),
    created_at_utc="2025-12-20T00:00:00Z",
)

# Register and compress
registry = InMemoryLexiconRegistry()
engine = LexiconEngine(registry)
engine.register(pack)

result = engine.compress(
    "slang",
    ["cap", "no_cap", "unknown"],
    run_id="RUN-123"
)
# result.matched == ("cap", "no_cap")
# result.weights_out == {"cap": 0.9, "no_cap": 1.1}
# result.provenance.inputs_hash â€” SHA256 of inputs
```

### Oracle Pipeline v1

Deterministic daily oracle generation from correlation deltas:

```python
from datetime import date
from abraxas.oracle import DeterministicOracleRunner, OracleConfig, CorrelationDelta

runner = DeterministicOracleRunner(git_sha="abc123", host="prod-01")
config = OracleConfig(half_life_hours=24.0, top_k=10)

deltas = [
    CorrelationDelta("slang", "crypto", "diamond_hands", 1.5, "2025-12-20T12:00:00Z"),
    CorrelationDelta("idiom", "tech", "move_fast", 0.7, "2025-12-19T18:00:00Z"),
]

artifact = runner.run_for_date(date(2025, 12, 20), deltas, config)
# artifact.output â€” ranked signals with decay weighting
# artifact.signature â€” deterministic SHA256 signature
# artifact.provenance â€” full execution metadata
```

**Features:**
- **Deterministic signatures** â€” Same inputs always produce same artifact signature
- **Time-weighted decay** â€” Recent signals weighted higher with configurable half-life
- **Provenance-embedded** â€” Every artifact includes inputs hash, config hash, git SHA
- **Modular design** â€” Composable transforms: decay, score_deltas, render_oracle
- **Golden test coverage** â€” 26 tests including signature stability verification

### Oracle v2 Governance Layer

Additive compliance and mode routing on top of v1 scoring:

```python
from abraxas.oracle.v2.wire import build_v2_block

# v2 block automatically added to oracle output
v2 = build_v2_block(
    checks={
        "v1_golden_pass_rate": 1.0,
        "drift_budget_violations": 0,
        "evidence_bundle_overflow_rate": 0.0,
        "ci_volatility_correlation": 0.72,
        "interaction_noise_rate": 0.22,
    },
    router_input={
        "max_band_width": 15.0,
        "max_MRS": 85.0,
        "negative_signal_alerts": 0,
        "thresholds": {"BW_HIGH": 20.0, "MRS_HIGH": 70.0},
    },
    config_hash="...",
)
# v2 contains: compliance (RED/YELLOW/GREEN), mode_decision (SNAPSHOT/ANALYST/RITUAL)
```

**Features:**
- **Compliance reporting** â€” Deterministic RED/YELLOW/GREEN status based on v1 regression checks
- **Mode routing** â€” Priority-based selection: user override â†’ compliance RED â†’ high uncertainty/risk â†’ default
- **Provenance lock** â€” Stable fingerprint for mode decision reproducibility
- **Additive-only** â€” v1 outputs preserved; v2 block appended to `output["v2"]`
- **Golden tests** â€” 5 deterministic tests for compliance and router logic

### Always-On Daemon

Run Abraxas as a persistent service:

- **Continuous Ingestion** â€” Scheduled scraping via Decodo API
- **Chat Interface** â€” LLM-like interaction with module discovery
- **Admin Handshake** â€” Dynamic capability detection
- **SQLite Storage** â€” Provenance-stamped document persistence

### Self-Healing Infrastructure

Production-grade reliability for edge deployment:

- **Drift Detection** â€” Git SHA, config, assets, dependencies tracking
- **Watchdog** â€” Automatic service restart on health check failures
- **Atomic Updates** â€” Zero-downtime deployments with rollback
- **Systemd Integration** â€” Managed lifecycle for Jetson Orin

---

## ğŸ“‹ Project Status

### âœ… Completed

- [x] **SCO/ECO Core** â€” Full symbolic compression detection pipeline
- [x] **Orin Boot Spine** â€” Edge infrastructure scaffolding
- [x] **TypeScript Integration** â€” Express API bridge to Python stack
- [x] **Weather Engine** â€” Signal transformation and narrative generation
- [x] **Always-On Daemon** â€” Ingestion engine and chat UI
- [x] **Self-Healing Layer** â€” Drift detection, watchdog, atomic updates
- [x] **Systemd Services** â€” Production deployment units
- [x] **Lexicon Engine v1** â€” Domain-scoped, versioned token-weight mapping
- [x] **Oracle Pipeline v1** â€” Deterministic oracle generation from correlation deltas
- [x] **Abraxas v1.4** â€” Temporal & Adversarial Expansion

### Abraxas v1.4: Temporal & Adversarial Expansion

**Version 1.4.0** introduces three foundational layers for temporal dynamics, adversarial resilience, and second-order narrative modeling:

#### Ï„ (Tau) Operator: Temporal Metrics

Three complementary temporal metrics for symbolic lifecycle tracking:

- **Ï„â‚• (Tau Half-Life)**: Symbolic persistence under declining reinforcement (hours)
- **Ï„áµ¥ (Tau Velocity)**: Emergence/decay slope from time-series (events/day)
- **Ï„â‚š (Tau Phase Proximity)**: Distance to next lifecycle boundary [0,1]

```python
from abraxas.core.temporal_tau import TauCalculator, Observation

calculator = TauCalculator(git_sha="abc123")
snapshot = calculator.compute_snapshot(observations, run_id="RUN-001")

print(f"Ï„â‚• = {snapshot.tau_half_life:.2f} hours")
print(f"Ï„áµ¥ = {snapshot.tau_velocity:.2f} events/day")
print(f"Confidence: {snapshot.confidence.value}")
```

#### D/M Layer: Information Integrity Metrics

Risk/likelihood estimators for information integrity assessment (NOT truth adjudication):

**Artifact Integrity**: PPS, PCS, MMS, SLS, EIS
**Narrative Manipulation**: FLS, EIL, OCS, RRS, MPS, CIS
**Network/Campaign**: CUS, SVS, BAS, MDS

**Composite Risk Indices**:
- **IRI** (Integrity Risk Index): [0,100]
- **MRI** (Manipulation Risk Index): [0,100]

```python
from abraxas.integrity import compute_composite_risk

risk = compute_composite_risk(artifact_integrity, narrative_manipulation, network_campaign)
print(f"IRI = {risk.iri:.1f}, MRI = {risk.mri:.1f}")
```

#### AAlmanac: Write-Once, Annotate-Only Ledger

Lifecycle state machine for symbolic evolution tracking:

**States**: Proto â†’ Front â†’ Saturated â†’ Dormant â†’ Archived

```python
from abraxas.slang.a_almanac_store import AAlmanacStore

store = AAlmanacStore()
term_id = store.create_entry_if_missing(term="cap", class_id="slang", ...)
state, tau = store.compute_current_state(term_id)
```

#### SOD (Second-Order Symbolic Dynamics)

Deterministic scaffolds for narrative cascade modeling:

- **NCP** (Narrative Cascade Predictor): Predicts cascade scenarios
- **CNF** (Counter-Narrative Forecaster): Generates counter-strategies
- **EFTE** (Epistemic Fatigue Threshold Engine): Models declining engagement
- **SPM** (Susceptibility Profile Mapper): Maps susceptibility profiles
- **RRM** (Recovery & Re-Stabilization Model): Models recovery trajectories

```python
from abraxas.sod import NarrativeCascadePredictor, SODInput

ncp = NarrativeCascadePredictor(top_k=5)
envelope = ncp.predict(sod_input, run_id="RUN-001")
```

#### Artifact Generators

Five specialized output formats:

- **Cascade Sheet**: Tabular summary of cascade paths
- **Manipulation Surface Map**: Heatmap data for D/M metrics
- **Contamination Advisory**: High-risk artifact alerts
- **Trust Drift Graph Data**: Time-series for Ï„â‚• and IRI/MRI
- **Oracle Delta Ledger**: Diff between current and prior snapshots

#### v1.4 CLI

```bash
python -m abraxas.cli.abx_run_v1_4 \
  --observations data/obs.json \
  --format both \
  --artifacts cascade_sheet,contamination_advisory \
  --output-dir data/runs/v1_4
```

**Features**:
- Delta-only mode (default): Emits only changed fields
- JSON/Markdown dual output
- Deterministic provenance embedding
- Confidence bands (LOW/MED/HIGH)

**Documentation**:
- [v1.4 Specification](docs/specs/v1_4_temporal_adversarial.md)
- [SOD Specification](docs/specs/sod_second_order_dynamics.md)
- [Canonical Ledger](docs/canon/ABRAXAS_CANON_LEDGER.txt)

### ğŸš§ In Progress

- [ ] **Real LLM Integration** â€” Replace stub chat engine with local/remote LLM
- [ ] **UI Dashboard** â€” React components for weather visualization
- [ ] **Expanded Lexicons** â€” Domain-specific compression dictionaries
- [ ] **PostgreSQL Migration** â€” Scale beyond SQLite for production
- [ ] **WebSocket Integration** â€” Real-time compression event streaming

### ğŸ¯ Roadmap

- [x] **Oracle Pipeline v2** â€” Governance layer with compliance reporting and deterministic mode routing (SNAPSHOT/ANALYST/RITUAL)
- [ ] **Ritual System** â€” Rune-based symbolic modulation
- [ ] **Multi-Domain Analysis** â€” Crypto, idiom, slang, technical jargon
- [ ] **Event Correlation** â€” Cross-domain drift pattern detection
- [ ] **Mobile UI** â€” Edge device management interface

### ğŸ“Š Recent Updates

See recent pull requests and commits:
- **#8** â€” Integrate Operator Auto-Synthesis (OAS) into Abraxas Slang System
- **#7** â€” Add always-on Abraxas daemon with Decodo ingestion and chat UI
- Pydantic dependency and OAS module integration
- Self-healing layer with watchdog and atomic updates

---

## ğŸ“– Documentation

### Core Modules

- **[SCO Stack](README_SCO.md)** â€” Symbolic Compression Operator documentation
- **[Orin Spine](README_ORIN.md)** â€” Edge deployment and infrastructure
- **[Integration Guide](INTEGRATION_SCO.md)** â€” TypeScript/Python integration
- **[Deployment Guide](DEPLOYMENT_SCO.md)** â€” Production deployment

### CLI Reference

```bash
# Orin commands
abx doctor          # System diagnostics
abx up              # Start HTTP server
abx smoke           # Run deterministic smoke test
abx assets sync     # Generate asset manifest
abx overlay list    # List installed overlays
abx drift check     # Check for configuration drift
abx watchdog        # Start health monitoring
abx update          # Atomic update with rollback
abx ingest          # Start data ingestion
abx ui              # Start chat UI server
abx admin           # Print admin handshake JSON

# SCO analysis
python -m abraxas.cli.sco_run --records <file> --lexicon <file> --out <file>
```

### API Endpoints

```bash
# Health checks
GET  /healthz              # Liveness
GET  /readyz               # Readiness with provenance

# SCO analysis
POST /api/sco/analyze      # Run compression detection
POST /api/sco/weather      # Generate weather signals
GET  /api/sco/lexicons     # List available lexicons

# Chat UI
GET  /admin/handshake      # Discover modules
POST /chat                 # Send messages
GET  /data/latest          # Inspect ingested data
```

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `ABX_ROOT` | `/opt/aal/abraxas` | Root installation directory |
| `ABX_PROFILE` | `orin` | Runtime profile (orin/dev) |
| `ABX_PORT` | `8765` | HTTP server port |
| `ABX_UI_PORT` | `8780` | Chat UI port |
| `ABX_DB` | `.aal/state/abx.sqlite` | SQLite database path |
| `DECODO_AUTH_B64` | (required) | Decodo API credentials |

### Systemd Services

For production deployment on Jetson Orin:

```bash
# Core services
sudo systemctl enable abraxas-core
sudo systemctl enable abx-ingest
sudo systemctl enable abx-ui
sudo systemctl enable abx-watchdog
sudo systemctl enable abx-update.timer

# Check status
sudo systemctl status abraxas-core abx-ingest abx-ui
```

---

## ğŸ§ª Testing

```bash
# Python tests
pytest tests/

# TypeScript tests
npm test
npm run test:coverage

# Smoke test (deterministic)
abx smoke

# E2E test
curl -X POST http://localhost:5000/api/sco/analyze \
  -H "Content-Type: application/json" \
  -d '{"texts": ["I love Aphex Twins"], "domain": "music"}'
```

---

## ğŸ¤ Contributing

Contributions welcome! This project follows deterministic, provenance-first design principles:

1. All changes must pass `abx smoke` deterministic tests
2. Include SHA-256 provenance for new linguistic events
3. Maintain backward compatibility for API endpoints
4. Follow existing code style (TypeScript/Python)

---

## ğŸ“„ License

MIT License â€” See [LICENSE](LICENSE) file for details.

---

## ğŸ”— Links

- **GitHub**: [scrimshawlife-ctrl/Abraxas](https://github.com/scrimshawlife-ctrl/Abraxas)
- **Issues**: [Report bugs or request features](https://github.com/scrimshawlife-ctrl/Abraxas/issues)
- **Decodo API**: [Web Scraping API](https://decodo.com)

---

<div align="center">

**Built for deterministic symbolic intelligence at the edge**

*Abraxas â€¢ Where language becomes weather*

</div>
