from __future__ import annotations

import argparse
import json
import os
import re
from collections import Counter
from datetime import datetime, timezone
from typing import Any, Dict, List, Tuple


def _utc_now_iso() -> str:
    return datetime.now(timezone.utc).replace(microsecond=0).isoformat()


def _read_jsonl(path: str) -> List[Dict[str, Any]]:
    if not path or not os.path.exists(path):
        return []
    out = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                d = json.loads(line)
                if isinstance(d, dict):
                    out.append(d)
            except Exception:
                continue
    return out


def _append_jsonl(path: str, obj: Dict[str, Any]) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")


def _write_json(path: str, obj: Dict[str, Any]) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)


TOKEN = re.compile(r"[A-Za-z][A-Za-z0-9_\-']{1,}")


def _tokens(text: str) -> List[str]:
    return [m.group(0).lower() for m in TOKEN.finditer(text or "")]


def _domain_group(domain: str) -> str:
    d = (domain or "").lower()
    if any(
        x in d
        for x in [
            "tiktok.com",
            "instagram.com",
            "youtube.com",
            "reddit.com",
            "twitter.com",
            "x.com",
            "facebook.com",
        ]
    ):
        return "social"
    if any(
        x in d
        for x in [
            "wikipedia.org",
            "nih.gov",
            "nature.com",
            "science.org",
            "arxiv.org",
            "acm.org",
            "ieee.org",
        ]
    ):
        return "reference"
    if any(
        x in d
        for x in [
            "reuters.com",
            "apnews.com",
            "bbc.co.uk",
            "nytimes.com",
            "wsj.com",
            "ft.com",
            "theguardian.com",
        ]
    ):
        return "mainstream"
    if not d:
        return "none"
    return "other"


PPS_PHRASES = [
    "share this",
    "spread the word",
    "wake up",
    "they don't want you to know",
    "mainstream media won't tell you",
    "proof",
    "undeniable",
    "100%",
    "always",
    "never",
    "traitor",
    "enemy",
    "evil",
    "corrupt",
    "globalist",
    "psyop",
    "false flag",
    "crisis actor",
    "plandemic",
]

MII_PHRASES = [
    "ai generated",
    "generated by ai",
    "deepfake",
    "synthetic",
    "voice clone",
    "face swap",
    "staged",
    "edited clip",
    "out of context",
    "doctored",
    "watermark",
    "no source",
    "source: trust me",
    "leaked screenshot",
]

CONSPIRACY_PHRASES = [
    "cover up",
    "they planned",
    "behind the scenes",
    "coordinated",
    "secret meeting",
    "backroom deal",
    "conspiracy",
    "plot",
    "operation",
    "intel",
    "agency",
    "black budget",
]

LOW_PROV_PHRASES = [
    "anonymous source",
    "sources say",
    "insider says",
    "rumor has it",
    "unconfirmed",
    "allegedly",
    "i heard",
    "someone said",
    "it is said",
    "trust me",
]


def _phrase_hits(text_l: str, phrases: List[str]) -> int:
    c = 0
    for p in phrases:
        if p in text_l:
            c += 1
    return c


def _normalize(x: float, cap: float) -> float:
    if cap <= 0:
        return 0.0
    return float(max(0.0, min(1.0, x / cap)))


def score_pps(text: str) -> Tuple[float, Dict[str, Any]]:
    tl = (text or "").lower()
    hits = _phrase_hits(tl, PPS_PHRASES)
    toks = _tokens(text)
    freq = Counter([t for t in toks if len(t) >= 4])
    rep = freq.most_common(1)[0][1] if freq else 0
    excls = tl.count("!")
    caps = sum(1 for w in re.findall(r"\b[A-Z]{3,}\b", text or ""))

    raw = hits + 0.5 * rep + 0.25 * excls + 0.5 * caps
    score = _normalize(raw, cap=10.0)
    return score, {"hits": hits, "rep": rep, "excls": excls, "caps_words": caps, "raw": raw}


def score_mii(text: str, domain: str) -> Tuple[float, Dict[str, Any]]:
    tl = (text or "").lower()
    hits = _phrase_hits(tl, MII_PHRASES)
    lowprov = _phrase_hits(tl, LOW_PROV_PHRASES)
    dg = _domain_group(domain)
    dom_boost = 1.0
    if dg in ("social", "other"):
        dom_boost = 1.15
    elif dg in ("reference", "mainstream"):
        dom_boost = 0.9
    raw = (hits * 1.4 + lowprov * 0.8) * dom_boost
    score = _normalize(raw, cap=8.0)
    return score, {
        "hits": hits,
        "lowprov_hits": lowprov,
        "domain_group": dg,
        "dom_boost": dom_boost,
        "raw": raw,
    }


def score_cas(text: str, pps: float, mii: float) -> Tuple[float, Dict[str, Any]]:
    tl = (text or "").lower()
    c_hits = _phrase_hits(tl, CONSPIRACY_PHRASES)
    coord = _normalize(float(c_hits), cap=4.0)
    manip = max(0.0, min(1.0, (pps + mii) / 2.0))
    if coord < 0.15:
        return 0.0, {"conspiracy_hits": c_hits, "coord": coord, "manip": manip}
    split = coord * (1.0 - 2.0 * manip)
    split = max(-1.0, min(1.0, split))
    return split, {"conspiracy_hits": c_hits, "coord": coord, "manip": manip}


def fronts_from_scores(pps: float, mii: float, cas: float) -> List[Dict[str, Any]]:
    fronts = []
    if mii >= 0.60:
        fronts.append(
            {
                "front_kind": "DEEPFAKE_SUSPECT",
                "strength": float(mii),
                "reason": "High MII (manipulation indicia).",
            }
        )
    if pps >= 0.65:
        fronts.append(
            {
                "front_kind": "PSYOP_PATTERN",
                "strength": float(pps),
                "reason": "High PPS (propaganda pressure).",
            }
        )
    if (mii >= 0.50) and (pps >= 0.55):
        fronts.append(
            {
                "front_kind": "POLLUTION",
                "strength": float((mii + pps) / 2.0),
                "reason": "Combined MII+PPS elevated.",
            }
        )
    if cas <= -0.35:
        fronts.append(
            {
                "front_kind": "POLLUTION",
                "strength": float(min(1.0, abs(cas))),
                "reason": "CAS indicates coordination-claim + high manipulation signals.",
            }
        )
    return fronts


def main() -> int:
    ap = argparse.ArgumentParser(
        description="WO-97: compute manipulation metrics (PPS/MII/CAS) over anchors and emit fronts"
    )
    ap.add_argument("--run-id", required=True)
    ap.add_argument("--anchor-ledger", default="out/ledger/anchor_ledger.jsonl")
    ap.add_argument("--metrics-ledger", default="out/ledger/manipulation_metrics.jsonl")
    ap.add_argument("--out", default="")
    ap.add_argument("--max-anchors", type=int, default=250)
    args = ap.parse_args()

    anchors = _read_jsonl(args.anchor_ledger)
    anchors = [a for a in anchors if isinstance(a, dict)]
    tail = anchors[-int(args.max_anchors) :]

    items = []
    for a in tail:
        aid = str(a.get("anchor_id") or "")
        dom = str(a.get("domain") or "")
        title = str(a.get("title") or "")
        hint = str(a.get("content_hint") or "")
        blob = (title + "\n" + hint).strip()
        if not blob:
            continue

        pps, pps_dbg = score_pps(blob)
        mii, mii_dbg = score_mii(blob, dom)
        cas, cas_dbg = score_cas(blob, pps, mii)
        fronts = fronts_from_scores(pps, mii, cas)

        ev = {
            "kind": "manipulation_metrics",
            "ts": _utc_now_iso(),
            "run_id": args.run_id,
            "anchor_id": aid,
            "domain": dom,
            "scores": {"PPS": float(pps), "MII": float(mii), "CAS": float(cas)},
            "debug": {"PPS": pps_dbg, "MII": mii_dbg, "CAS": cas_dbg},
            "fronts": fronts,
            "notes": "WO-97 heuristics: flags only; not a verdict. No censorship.",
        }
        _append_jsonl(args.metrics_ledger, ev)
        items.append(ev)

    stamp = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
    out_path = args.out or os.path.join(
        "out/reports", f"manipulation_metrics_{stamp}.json"
    )
    _write_json(
        out_path,
        {
            "version": "manipulation_metrics_report.v0.1",
            "ts": _utc_now_iso(),
            "run_id": args.run_id,
            "n": len(items),
            "items": items[-120:],
            "notes": "Materialized report; full history in manipulation_metrics.jsonl",
        },
    )
    print(f"[MANIP] wrote: {out_path} n={len(items)}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
